{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e726d57-7a4d-4751-b54f-0129f5fd5267",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "What is a Random Forest:\n",
    "\n",
    "A Random Forest is an ensemble learning method that builds multiple decision trees and combines their outputs to make a more accurate and stable prediction.\n",
    "\n",
    "How it works:\n",
    "It creates many decision trees using random subsets of your data (bootstrapping).\n",
    "\n",
    "At each split in each tree, it chooses from a random subset of features.\n",
    "\n",
    "For classification, it uses majority voting (i.e., the class most trees agree on).\n",
    "\n",
    "For regression, it uses the average of the predictions.\n",
    "\n",
    "When to Use Random Forest:\n",
    "Use Random Forest when:\n",
    "\n",
    "You want a robust model that avoids overfitting.\n",
    "\n",
    "You have noisy data or high dimensionality (many features).\n",
    "\n",
    "You want feature importance insights.\n",
    "\n",
    "You want a balance of accuracy and interpretability.\n",
    "\n",
    "\n",
    "`The parameter n_estimators=100 specifies the number of decision trees that the Random Forest will build to make its final prediction.`\n",
    "\n",
    "\"\"\" Explanation in Simple Terms:\n",
    "A Random Forest is made up of many individual Decision Trees.\n",
    "\n",
    "The more trees you include (i.e., the higher the n_estimators), the more stable, robust, and accurate your model tends to be — up to a point.\n",
    "\n",
    "Each tree is trained on a slightly different random subset of the data, and then during prediction, the forest:\n",
    "\n",
    "Votes (for classification)\n",
    "\n",
    "Averages (for regression) \"\"\"\n",
    "\n",
    "\n",
    "\"\"\" So Why 100?\n",
    "100 is a default value in many cases and often provides a good balance between performance and training time.\n",
    "\n",
    "You can increase this number for better accuracy (especially with larger datasets), but:\n",
    "\n",
    "It will take more time to train.\n",
    "\n",
    "It will use more memory. \"\"\"\n",
    "\n",
    "rf_model = RandomForestClassifier(n_estimators=10) fewer trees: faster, less accurate\n",
    "rf_model = RandomForestClassifier(n_estimators=100) standard setting\n",
    "rf_model = RandomForestClassifier(n_estimators=500) more stable but slower\n",
    "\n",
    "Rule of Thumb:\n",
    "Small datasets: n_estimators=50-100 is usually enough.\n",
    "\n",
    "Large datasets: You can use n_estimators=200-500+, depending on your system and needs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4ac539-de30-4b97-9f4a-c3b4ba174928",
   "metadata": {},
   "source": [
    "Advantages of Random Forest:\n",
    "Handles missing data and categorical features well (with some prep)\n",
    "\n",
    "Reduces overfitting from individual trees\n",
    "\n",
    "Good for both classification and regression\n",
    "\n",
    "Provides feature importance\n",
    "\n",
    "Disadvantages of Random Forest: \n",
    "Slower than single decision trees\n",
    "\n",
    "Less interpretable than one tree\n",
    "\n",
    "Large models can take more memory\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Focus on Relevant Features (Feature Selection)\n",
    "We selected these features because they are the most directly related to whether someone survived or not (survived is the target). Including irrelevant or less informative features can:\n",
    "\n",
    "Introduce noise\n",
    "\n",
    "Make the model overfit\n",
    "\n",
    "Complicate training without improving performance\n",
    "\n",
    "\n",
    "| Abbreviation | Full Name      | Meaning                                           |\n",
    "| ------------ | -------------- | ------------------------------------------------- |\n",
    "| **TN**       | True Negative  | Model correctly predicted \"Not Survived\"          |\n",
    "| **FP**       | False Positive | Model predicted \"Survived\" but was \"Not Survived\" |\n",
    "| **FN**       | False Negative | Model predicted \"Not Survived\" but was \"Survived\" |\n",
    "| **TP**       | True Positive  | Model correctly predicted \"Survived\"              |\n",
    "\n",
    "\n",
    "\n",
    " What Is Feature Importance?\n",
    "In a Random Forest model, feature_importances_ tells you which features are most useful in making predictions.\n",
    "\n",
    "It measures how much each feature reduces uncertainty (e.g., Gini impurity or entropy) in decision trees — on average across all trees in the forest.\n",
    "\n",
    "Higher value → Feature is more important for prediction.\n",
    "\n",
    "Lower value → Feature contributes little.\n",
    "\n",
    "\n",
    "Step-by-Step Explanation\n",
    "importances = rf_model.feature_importances_\n",
    "This extracts a list of numerical importance scores for each feature used in training the RandomForestClassifier.\n",
    "\n",
    "\n",
    "Why Use Feature Importance?\n",
    "To understand which features matter most.\n",
    "\n",
    "To simplify your model by removing weak features.\n",
    "\n",
    "To help interpret your machine learning model.\n",
    "\n",
    "\n",
    "| Feature    | Importance |\n",
    "| ---------- | ---------- |\n",
    "| `sex`      | 0.40       |\n",
    "| `fare`     | 0.30       |\n",
    "| `age`      | 0.15       |\n",
    "| `pclass`   | 0.10       |\n",
    "| `embarked` | 0.05       |\n",
    "\n",
    "You can say:\n",
    "\n",
    "The model relies most on sex and fare to predict survival.\n",
    "\n",
    "You might even consider removing embarked if it's contributing little."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd85d42-ddcb-4c19-8b87-3b2a6cb469b6",
   "metadata": {},
   "source": [
    "Label Encoding convert categorical columns (text data) into numerical format so they can be used in a machine learning model like Random Forest, which only works with numerical input.\n",
    "\n",
    "What this loop does:\n",
    "For each column in the list label_cols, it does:\n",
    "\n",
    "data[col].astype(str): Ensures that all values are strings. This avoids errors when there are missing values (NaN) or mixed types.\n",
    "\n",
    "le.fit_transform(...): Converts each unique category into an integer (e.g., 'Yes' → 1, 'No' → 0, 'N' → 0, 'S' → 1, etc.).\n",
    "\n",
    "data[col] = ...: Replaces the original string values in the column with their numeric codes.\n",
    "\n",
    "Why This Is Important\n",
    "ML algorithms need numeric input: Random Forest, Logistic Regression, etc., don’t understand text.\n",
    "\n",
    "Label Encoding is fast and simple: It works well when the categorical values have no inherent order.\n",
    "\n",
    "When to Use LabelEncoder (and When Not To)\n",
    "Good Use Case:\n",
    "Binary categories like 'Yes'/'No', 'Male'/'Female'\n",
    "\n",
    "High cardinality categorical features when used with tree-based models (e.g., Random Forest)\n",
    "\n",
    "Be Careful When:\n",
    "Using ordinal encoding where the order matters ('Low', 'Medium', 'High')\n",
    "\n",
    "Using linear models (e.g., Logistic Regression) → LabelEncoding may introduce fake order → use OneHotEncoding instead.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
